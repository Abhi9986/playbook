{
  "1": {
    "id": "1",
    "title": "API Standards",
    "content": "API Standards REACH ALL BY DIVISION BY STACK LEADS Domain Owner API Integration     An API is not the implementation of a web service. It is the specification of the interface to the webservice; where it is and how it works. An API should be reusable by other services. A well-designed web API should aim to support: Platform independence. APIs should be reusable, any client should be able to call the API, regardless of how the API is implemented internally. An API should follow the standard protocols and ideally shared through the Qantas API Gateway. Service evolution. An API should be able to evolve and add functionality independently from client applications. As the API evolves, existing client applications should continue to function without modification (breaking changes should only occur in extreme cases). All functionality should be discoverable, so that client applications can fully utilise it. Resource Naming Use nouns over verbs in resource names Use plural nouns in all cases. Example: /customers instead of /customer Differentiate between the collection resource and the singular resource. Example: /customers is the collection, /customers/1234 is a single entity Everything is a resource, rethink interactions in terms of resources not operations Use correct http verbs (POST, PUT, PATCH, DELETE, HEAD, OPTIONS) Collection Resources Collections should support some standard operations: Paging: Standard query parameter page, used as necessary depending on the paging strategy (page based, offset based or cursor based. Example: /customers? page[offset]=100&amp;page[size]=10 Sorting: Standard query parameter sort with comma-delimited list of fields to sort on, with + and - for order. Example: /customers?sort=+age,+name Filtering: Standard query parameter filter, used as necessary for filtering fields, with a variety of strategies (equality, range checks etc). Example: /customers?filter[age:gt]=30&amp;filter[postcode:eq]=2000 Collections should return commonly useful data for each found resource Singular Resources Singular resources should support some standard operations: Sparse fields: Standard query parameter fields is a comma-delimited list of fields to return rather than the whole resource Example: /customers/1234?fields=name,dob Expanding: Standard query parameter include lists subresources that should be embedded in the main resource. Example: /customers/1234?include=accounts,owner Secondary identifiers: Allow users to provide non-primary identifiers to simplify lookups. Example: /customers/safi:abcdefg Resource Paths /customers for a collection of entities /customers/[id] for single entity /customers/[id]/bookings for a collection of entities belongs to a given entity Response Codes Use HTTP responses correctly in all cases based on the below spec: [https://tools.ietf.org/html/rfc7231#section-6 200 for successful, synchronous operations (usually GET, PUT, PATCH or DELETE) 201 for successful, synchronous operations that result in a new resource (usually POST) 202 for successful, asynchronous operations, returning a resource that be polled for status (usually POST) 401 for unauthenticated 403 for unauthorized 412 for invalid request 400 for bad data in the request 404 for resources that do not exist [418]{.underline} for I’m a teapot 500 for service unavailability Documentation Use [Swagger 2.0 for documenting API.",
    "url": "http://localhost:4000/playbook/docs/Standards/APIStandards/",
    "relUrl": "/docs/Standards/APIStandards/"
  },
  "2": {
    "id": "2",
    "title": "Agile",
    "content": "Agile REACH ALL BY DIVISION BY STACK LEADS Head of Software Engineering     We have a bias for action. Teams should focus on delivering quality software quickly, testing, learning and iterating. Best practice agile is subjective, we aim to listen, learn and improve over every iteration. Agile frameworks such as Scrum and Kanban are selected at the discretion of the team based upon their requirements. At minimum, the following ceremonies must occur: Estimations Daily stand-up Sprint Kick-offs (in Scrum) Showcase Retrospectives",
    "url": "http://localhost:4000/playbook/docs/agile",
    "relUrl": "/docs/agile"
  },
  "3": {
    "id": "3",
    "title": "Bugs",
    "content": "Bugs Useful bug reports are ones that get bugs fixed. A useful bug report is... Reproducible - If an Engineer can&#39;t see it or conclusively prove that it exists, the more likely it will be returned &quot;It works for me!&quot; Specific - The quicker the Engineer can trace down the issue to a specific problem, the more likely it&#39;ll be fixed expediently. The goals of a bug report are to: Pinpoint the bug Explain it to the QA team to triage How to write a good bug report A good bug report should be brief but include the following information: Summary A brief summary of the issue Steps to Reproduce The goal of reproducible steps is to teach developer to recreate the bug on his own system. It may be as simple as Load the attached test case in Browser XYZ. A more complex case may involve multiple steps, such as: Step 1: Load the attached test case in Browser XYZ Step 2: Scroll to the bottom of the page Step 3: Click the link Step 4: Page should show X but shows Y  Expected Result Detail what you expect the result to be.  Actual Result A picture tells a thousand works and saves a lot of bug writing. Attached screenshots or record behaviour (e.g. http://recordit.co/) to help quickly triage the issue.  Impact Scope: A measure for how many people it affects, usually on a scale of 1 to 5. So if everyone’s completion rate is 5% higher than it should be, that would be a 5, as it affects everyone. It’s really good if you have some description for this, e.g.: Level 1: Few reported cases Level 2: Affects a small sub-sector of users (&lt;20% or something) Level 3: Affects a medium sub-sector of users (20 - 50% ish) Level 4: Affects all posters OR all workers Level 5: Affects all users Severity:  A measure for how badly it affects the user and/or business. Potential scale could be: Level 1: A typo (unlikely to have customer service impacts), duplicate notifications Level 2: Incorrect completion rate (some people might write in about it), discrepancy in task count Level 3: Something that is stopping people from messaging, making an offer, or assigning Level 4: Payments incorrectly processed, e.g. the bug from the other week Level 5: Whole site down/Armageddon Under this, it’s also important to note the impacts, for example: Increased ticket volume Manual workaround uses up customer service time etc. Lost revenue Negative PR",
    "url": "http://localhost:4000/playbook/docs/Agile/Bugs/",
    "relUrl": "/docs/Agile/Bugs/"
  },
  "4": {
    "id": "4",
    "title": "CI/CD",
    "content": "CI/CD REACH ALL BY DIVISION BY STACK LEADS Paul Keen     Continuous integration, delivery, and deployment, known collectively as CI/CD, is an integral part of modern development intended to reduce errors during integration and deployment while increasing project velocity. CI/CD is a philosophy and set of practices often augmented by robust tooling that emphasize automated testing at each stage of the software pipeline. By incorporating these ideas into your practice, you can reduce the time required to integrate changes for a release and thoroughly test each change before moving it into production. At Qantas: Atlassian Bamboo is used as the CI Pipeline All builds must use the CI/CD Pipeline Access to the Pipeline configuration is managed through trusted AD user groups. All builds must be commended by a pull request. No pull request no build Must have a build break practice. Never deploy a red build. Martin Fowlers CI/CD Principals Maintain a Single Source Repository. Automate the Build Make Your Build Self-Testing Everyone Commits To the Mainline Every Day Every Commit Should Build the Mainline on an Integration Machine Fix Broken Builds Immediately Keep the Build Fast Test in a Clone of the Production Environment Make it Easy for Anyone to Get the Latest Executable Everyone can see what&#39;s happening Automate Deployment More Info https://martinfowler.com/articles/continuousIntegration.html",
    "url": "http://localhost:4000/playbook/docs/Standards/CICD/",
    "relUrl": "/docs/Standards/CICD/"
  },
  "5": {
    "id": "5",
    "title": "Ceremonies",
    "content": "Ceremonies All ceremonies (stand-up, chapter and guilds) are open meetings available for all to attend. We believe that everyone’s opinion is equal and everyone gets to have their say. However, we do not need consensus to move forward. To maintain transparency, all chapter and guild agendas, the lead must: Post the agenda in an appropriate public Slack channel Document the meeting minutes in Confluence Post the meeting minutes link in the same Slack channel as the agenda",
    "url": "http://localhost:4000/playbook/docs/OperationalExcellence/Ceremonies/",
    "relUrl": "/docs/OperationalExcellence/Ceremonies/"
  },
  "6": {
    "id": "6",
    "title": "Characteristics of a Good Unit Test",
    "content": "Characteristics of a Good Unit Test A good unit test has the following characteristics: Runs fast, runs fast, runs fast. If the tests are slow, they will not be run often. Separates or simulates environmental dependencies such as databases, file systems, networks, queues, and so on. Tests that exercise these will not run fast, and a failure does not give meaningful feedback about what the problem actually is. Is very limited in scope. If the test fails, it&#39;s obvious where to look for the problem. Use few Assert calls so that the offending code is obvious. It&#39;s important to only test one thing in a single test. Runs and passes in isolation. If the tests require special environmental setup or fail unexpectedly, then they are not good unit tests. Change them for simplicity and reliability. Tests should run and pass on any machine. The &quot;works on my box&quot; excuse doesn&#39;t work. Often uses stubs and mock objects. If the code being tested typically calls out to a database or file system, these dependencies must be simulated, or mocked. These dependencies will ordinarily be abstracted away by using interfaces. Clearly reveals its intention. Another developer can look at the test and understand what is expected of the production code. Reference: https://msdn.microsoft.com/en-us/library/aa730844(v=vs.80).aspx",
    "url": "http://localhost:4000/playbook/docs/Testing/CharacteristicsUnitTest/",
    "relUrl": "/docs/Testing/CharacteristicsUnitTest/"
  },
  "7": {
    "id": "7",
    "title": "Cloud",
    "content": "Cloud REACH ALL BY DIVISION BY STACK LEADS Head of Cloud     Instance Sizing: Select your cloud platform provider. Select a use case category that most closely describes your expected workload. Some use cases correspond to particular SoftNAS Cloud product recommendations. Review the resource properties for the chosen category to confirm they match your expected level of instance resources. If unsure of your final selection, start with a larger instance size initially, deploy or simulate your production workload, then monitor and observe your results for 30 to 45 days. If your actual workload peaks are less than 50% CPU and network usage, then you may be able to reduce the instance size such that the peaks do not exceed 70%. If CPU or network usage reaches 75% or higher, then an increase in instance size may be appropriate.",
    "url": "http://localhost:4000/playbook/docs/Standards/Cloud/",
    "relUrl": "/docs/Standards/Cloud/"
  },
  "8": {
    "id": "8",
    "title": "Code Review/Pull Requests",
    "content": "Code Reviews/Pull Requests All production code needs to be reviewed via a pull request. Code reviews are classless: seniority is not required for a peer review although repository ownership can mandate approver authority. The aim of pull requests are to: Catch major defects before they are introduced into the existing code base Discuss architectural improvements and provide guidance for changes Look for secure coding practices Establish and follow coding standards Give kudos for great work Review Unit Tests Learn from other developers Cross Skill yourself Pull Request Social Contract Before each review, the Social Contract should be considered: We respect each other at all times We assume good intent Comments are contextual; focused only on the code Every comment should be addressed, never ignored Every peer review is an opportunity of collective learning and collaboration Pull Request Preparation Pull Requests should meet the following criteria: Should have description of changes Pull requests should be as small as possible with limited numbers of scope changes Build must be green and passed tests Reviewers should mark the pull request as “need work” even if they think the proposed changes are minor Should have description of changes in pull request Include all who are involved with that project Should squash unmeaningful commit messages before merge into master Include a brief on what the change is about. Should include unit test Pull Request Reviewers The guidelines for Pull Request Reviewers are: Mandatory 2 approvers (should not include members if pair programmed) Ideally 1 reviewer outside your team. Reviewed within two working days Requested refactoring changes should not change behaviour",
    "url": "http://localhost:4000/playbook/docs/Standards/CodeReview/",
    "relUrl": "/docs/Standards/CodeReview/"
  },
  "9": {
    "id": "9",
    "title": "Code Style",
    "content": "Code Style REACH ALL BY DIVISION BY STACK LEADS Head of Software Engineering     We write code in a consistent style. This ensures that all team members use the same dialect when writing code, helping us to communicate with each other. Use prefer automated style-guide enforcements (linting built into the build pipeline, for example) over style enforcement during reviews. A combination of automated tools, manual reviews, and automated formatting will ensure the code gets to production with the right style. Style Guides Please use the following style guides: HTML and CSS React/JSX style guide C# Style Guide Swift style guide Go style guide Kotlin style guide Lint Tools Before starting developing code, ensure you have linters installed and configured for your code editor. C++ Style Guide, Objective-C Style Guide Java Style Guide Python Style Guide R Style Guide Shell Style Guide HTML/CSS Style Guide JavaScript Style Guide AngularJS Style Guide Common Lisp Style Guide Vimscript Style Guide",
    "url": "http://localhost:4000/playbook/docs/Standards/CodeStyle/",
    "relUrl": "/docs/Standards/CodeStyle/"
  },
  "10": {
    "id": "10",
    "title": "Confluence",
    "content": "Confluence Confluence should be used for all technical documentation. User Stories User Stories are business requirements, told from a user’s point of view, and representing business value. They describe something a user would like to achieve or a problem they want to solve. How to write good user stories We believe in conversation over documentation, but user stories have minimum requirements: Acceptance criteria: bullet point list of the requirements Imagery: Attached image or link where appropriate For more complex stories: Scenarios: good and bad scenarios of what the user/system is supposed to perform A good user story should be: Business focused - A story should be written in terminology that the business understands; it must return business value. Elevatory friendly - A story is a placeholder for a more detailed conversation, not a feature spec or requirements doc. You should be able to explain the story on a 30 second elevator ride. Testable - A story should be testable - if you can&#39;t test it you shouldn&#39;t build it Note: A user story should take no more than 20 minutes to write.  Acceptance Criteria Acceptance Criteria are a set of statements, each with a clear pass/fail result, that specify both functional (e.g., minimal marketable functionality) and non-functional (e.g., minimal quality) requirements applicable. They should be high-level while still providing enough detail to be useful. They should include: Functional Criteria: Identify specific user tasks, functions or business processes that must be in place. A functional criterion might be “A user is able to access a list of available reports.” A non-functional criterion might be “Edit buttons and Workflow buttons comply with the Site Button Design.” Non-functional Criteria: Identify specific non-functional conditions the implementation must meet, such as design elements. A non-functional criterion might be “Edit buttons and Workflow buttons comply with the Site Button Design.” Performance Criteria: If specific performance is critical to the acceptance of a user story, it should be included. This is often measured as a response time, and should be spelled out as a threshold Acceptance Criteria should state intent, but not a solution (e.g., “A manager can approve or disapprove an audit form” rather than “A manager can click an ‘Approve/Disapprove’ radio button to approve an audit form”). The criteria should be independent of the implementation: ideally the phrasing should be the same regardless of target platform. Example:  Customer can enter coupon code at the payment stage Coupon code will adjust price either by $$ or % Customer can pay by the new discounted amount  Scenarios A scenario describes the steps necessary to test a story, bug or tech debt. Stories, bugs and tech debt should include multiple scenarios both the happy path and alternate paths. It is equally important to describe &quot;happy-day&quot; scenarios as well as &quot;bad-day&quot; scenarios where unexpected user or system behaviour occur. The essential idea is to break down writing a scenario (or test) into three sections: The Given part describes the state of the world before you begin the behaviour you&#39;re specifying in this scenario. You can think of it as the pre-conditions to the test. The When section is that behaviour that you&#39;re specifying. Finally the Then section describes the changes you expect due to the specified behaviour. Scenario 1: Title Given [context] 1. And [:some more context]… 2. When [event] 3. Then [:outcome] 4. And [another outcome]… Example:  Scenario: Happy-day Given when a Customer has a coupon code at the payment stage,  When the code entered is correct And within Expire Then apply discount  When the code entered is incorrect or expired Then show error message Scenario: Campaign expired (bad-day) Given when a Customer has a coupon code at the payment stage,  When the code entered is correct And the campaign date &lt; TODAY Then show error message Scenario: Negative payment value (bad-day) Given when a Customer has a coupon code at the assign task stage,  When the discount is below fare value Then credit to zero but not negative. The Given, When, Then syntax is useful for testing purposes but not mandatory. If you prefer a more storytelling scenario format that depicts the intention of the feature, then by all means write in that format.",
    "url": "http://localhost:4000/playbook/docs/Agile/Confluence/",
    "relUrl": "/docs/Agile/Confluence/"
  },
  "11": {
    "id": "11",
    "title": "Credentials",
    "content": "Credentials Production credentials, keys or other secrets should not be stored in plain text in any code or testing script. Further, credentials should not be known by Developers; all credentials should be stored in a secrets management solution. Secrets should be rotated on a minimum of a quarterly basis, ideally through an automated process. Credentials for Dev, UAT and Production environments must be different.",
    "url": "http://localhost:4000/playbook/docs/Security/Credentials/",
    "relUrl": "/docs/Security/Credentials/"
  },
  "12": {
    "id": "12",
    "title": "Definitions of Done",
    "content": "Definitions of Done REACH ALL BY DIVISION BY STACK LEADS Head of Software Engineering     A task is not considered ‘done’ until it has satisfied the following criteria: All acceptance criteria have been demonstrated and approved Automated tests cover changed functionality are included The story has been verified and approved by QA Secure coding standards, practices and policies for the story have been met The feature/code has been code reviewed and refactored (if required) Pull request notes have been responded to adequately Release notes completed Tech debt has not increased The wiki is updated (if necessary)",
    "url": "http://localhost:4000/playbook/docs/Standards/Definitions/",
    "relUrl": "/docs/Standards/Definitions/"
  },
  "13": {
    "id": "13",
    "title": "Design Principles",
    "content": "Design Principles Follow the SOLID principles, where possible: Single Responsibility Principle Open/Closed Principle Liskov Substitution Principle Interface Segregation Principle Dependency Inversion We also advocate the advice of Clean Code Bad code does too much – Clean code is focused The language you wrote your code with should look like it was made for the problem. It is not the language that makes a program look simple, but the programmer who makes the language appear simple. It should not be redundant. Comply with DRY principles, although don’t become zealous on your implementation of this. Reading your code should be pleasant Can be easily extended by any other developer. Similarly, code should easily be removed It should have minimal dependencies Smaller is better",
    "url": "http://localhost:4000/playbook/docs/Standards/DesignPrinciples/",
    "relUrl": "/docs/Standards/DesignPrinciples/"
  },
  "14": {
    "id": "14",
    "title": "Guilds",
    "content": "Guilds Guilds are formed around “community of interest”, a group of people that want to share knowledge, tools, code, and practices. Guilds should be seen as semi-permanent with assigned leaders who manage the health of the Guild. iOS LEADS Razvan Balazs, Jamie Sciban FREQUENCY Fortnightly SLACK CHANNEL #ios-guild CONFLUENCE   Android LEADS Xiang Kong FREQUENCY Fortnightly SLACK CHANNEL #apps-android CONFLUENCE   Java LEADS Diana Sullstyo FREQUENCY Fortnightly SLACK CHANNEL TBA CONFLUENCE   Front End LEADS Theo Hatzi FREQUENCY Fortnightly SLACK CHANNEL https://qantas.slack.com/messages/C5GEX3XG8 CONFLUENCE TBA",
    "url": "http://localhost:4000/playbook/docs/OperationalExcellence/Guilds/",
    "relUrl": "/docs/OperationalExcellence/Guilds/"
  },
  "15": {
    "id": "15",
    "title": "Jira",
    "content": "JIRA All development activity is scoped and tracked via Atlassian Jira. Jira is a tool to enable the team to work effectively therefore the workflow implemented should complement the teams working style. Boards, columns status may be different per team however at a minimum: Each team will have a Jira project Each Jira project will have one or multiple boards Each board/s will have columns and status as per their workflow Physical boards are used at the discretion of the team however, it is a requirement that all tasks have an associated Jira ticket. No ticket, no work. Jira location: http://jira.qantas.com.au The minimum requirements of types are: Epic Story Bug Task All Stories should be identifiable with one of the following Labels or tags. Product (Normal Feature Development) BAU (Build Pipeline Maintenance, Patching, Version Upgrades) Tech Debt Innovation Production Support The minimum workflow steps should be: Backlog In Dev Ready for Code Review Testing Ready for Release Done",
    "url": "http://localhost:4000/playbook/docs/Agile/Jira/",
    "relUrl": "/docs/Agile/Jira/"
  },
  "16": {
    "id": "16",
    "title": "Logging",
    "content": "Logging REACH ALL BY DIVISION BY STACK LEADS Mng Application Performance     Logs play an important role in development, allowing Developers and DevOps to diagnose problems in an application before and after a release; it allows Cyber to inspect and analyse behaviour and help build metrics to ascertain platform health. A strong logging strategy should be considered pivotal to the success of your application. Note: No PI data should every be stored in logs Logging Traffic Lights Aside from standard diagnosing application issues, logging hooks should be written into your application from which to build monitoring and alerting. Logging events should be based upon a traffic light principle. Red (fatal &amp; error): fatal or unexpected values in the application. Example: the application encounters an error which is preventing a particular request from completing Amber (warn): exceptions or unusual behaviour that may forebode future failure. Example: object detects inconsistency within an external data feed and it performs a corrective action to compensate for it. Green (info &amp; trace): log key events in the system or user journey to demonstrate application health. Example: registration, booking and confirmation in a booking flow Logging Standard We recommend you use the following logging standard in a key/value pair that is easily searchable in Splunk. Timestamp: Date and time in UTC ISO8601 e.g. 20180915T155300 Correlation id: e.g. session id Action: e.g. &quot;GET /libs/granite/csrf/token.json HTTP/1.1&quot; Status: in http error format e.g. 200 Reason: human readable reason Level: e.g. INFO Message: The stack trace Logging Levels FATAL: Events that indicate problems requiring immediate attention. There may be different criticality of errors. The assumption at this point is that the alarming built in to Splunk will be configured to reflect. ERROR: Unhandled events that provide forewarning of potential problems. This level should be used to log unhandled exceptions which are caught by a catch block at the boundary of the application which catches all occurring exceptions that have not been handled by other exception handlers. WARN: Handled events that provide forewarning of potential problems; although not a response to an actual error, a warning indicates that a component or application is not in an ideal state and that some further actions could result in a critical error. This level should be used for handled exceptions. For example the application requires a configuration setting but has a default in case the setting is missing. INFO: All events which are required for audit purposes DEBUG: Events that perform &quot;normal operations&quot;, e.g. mail sent, user updated profile etc. TRACE: All other events: Executed queries, user authentication, session expired messages Begin method X, end method X etc. Any event that could help with debugging Note: To meet audit requirements the default setting should be level INFO.",
    "url": "http://localhost:4000/playbook/docs/Standards/Logging/",
    "relUrl": "/docs/Standards/Logging/"
  },
  "17": {
    "id": "17",
    "title": "Operational Excellence",
    "content": "Operational Excellence REACH ALL BY DIVISION BY STACK LEADS Head of Software Engineering     Delivering a feature to production is just first milestone in delivering value for our customers, but it is definitely not the end of the journey. Once a feature goes live, we need to be able to monitor, maintain and support it.",
    "url": "http://localhost:4000/playbook/docs/operationalexcellence",
    "relUrl": "/docs/operationalexcellence"
  },
  "18": {
    "id": "18",
    "title": "People",
    "content": "People REACH ALL BY DIVISION BY STACK LEADS Head of Software Engineering     Onboarding Effective onboarding allows our valuable new talent to embed themselves in our business quickly and sets them up for success. Complete the following checklist for all new starters: Do Developers have access to all systems, accounts and equipment before they start? Have new starters been assigned a buddy? Have new starters been introduced to key stakeholders and met with their team? Have new starters been taken through a product/domain walkthrough? Do Developers have a fully functional dev environment before lunch? Can Developers draw a high-level architecture by the end of the day? Do Developers write and commit non-trivial code on day one? Do new starters contribute to the onboarding process? Do new starters feel more excited about Qantas and their role to play by the end of the day? It is expected our partners build an institutional knowledge about the ways of work at Qantas and provided to new starters before they start. Learning We use a T-shaped learning strategy at Qantas. We expect a broad understanding across the technology landscape (horizontal). We also expect Developers to specialise in one or a few technologies or technical (verticals). Developers are part of the greater Qantas community with many opportunities to collaborate, learn and mentor with. Some of the activities include: TechHangar: Monthly forum to share knowledge about interesting technologies, trends, projects, practices, etc available in per person or Skype. Sessions are recorded and stored in the Qantas wiki. Hackathons: Performed quarterly. These are theme-based and we encourage all staff to participate to innovate to build and test new ideas focused on Qantas goals. Cross functional teams can be formed to help understand the problem (including Product/UX/Engineering/Support) MeetUps: external and occasionally hosted on campus Coding tournaments such as Secure Code Warrior Partner conferences Offboarding We hate to see people leave, doing it well leaves a lasting impression. Exit interviews: an exit interview should be given to all team members. This is the opportunity for the person leaving to provide feedback. We encourage one-way advice. Handovers: ensure knowledge, access and contacts are handed over Access removal: remove access to all systems, especially those not covered under SSO. All Software Engineers with access to Qantas systems must inform Qantas on the Developers offboarding date prior to them leaving. Evangelism Finally, we believe it is the role of every Software Engineer to be proud of the work you do and even more so, proud of the team you work with. We believe each individual should evangelise these great efforts at MeetUps, conferences, guest blogs and BBQs. =",
    "url": "http://localhost:4000/playbook/docs/People/People/",
    "relUrl": "/docs/People/People/"
  },
  "19": {
    "id": "19",
    "title": "Principles",
    "content": "Principles REACH ALL BY DIVISION BY STACK LEADS Head of Software Engineering     What does good look like? We benchmark our teams against the following questions. These form our principles on what makes a high performing team. Are you proud of every pull request? Do you have cross-functional teams that scope, build, test and deploy together? Do you enforce documented coding standards? Do you have the best tools to maximise your productivity? Do you have automated test coverage for all key workflows? Do you automate continuous delivery and deployment? Do you support the code you build? Do you allocate time and resources to technical debt? Do you actively monitor and create alerts for your applications in production? Is security, customer privacy and safety your primary concern? Do you invest time and resources in training? Do you actively contribute to the software engineering culture and evangelise our efforts? Credit: https://www.joelonsoftware.com/2000/08/09/the-joel-test-12-steps-to-better-code/",
    "url": "http://localhost:4000/playbook/docs/principles",
    "relUrl": "/docs/principles"
  },
  "20": {
    "id": "20",
    "title": "README",
    "content": "README REACH ALL BY DIVISION BY STACK LEADS Head of Software Engineering     All repositories must content a README. Use this template for example content: # Project Title One Paragraph of project description goes here ## Getting Started These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system. ### Prerequisites What things you need to install the software and how to install them Give examples ### Installing A step by step series of examples that tell you how to get a development env running Say what the step will be Give the example And repeat until finished End with an example of getting some data out of the system or using it for a little demo ## Running the tests Explain how to run the automated tests for this system ### Break down into end to end tests Explain what these tests test and why Give an example ### And coding style tests Explain what these tests test and why Give an example ## Deployment Add additional notes about how to deploy this on a live system ## Built With * [Dropwizard](http://www.dropwizard.io/1.0.2/docs/) - The web framework used * [Maven](https://maven.apache.org/) - Dependency Management * [ROME](https://rometools.github.io/rome/) - Used to generate RSS Feeds ## Contributing Please read [CONTRIBUTING.md](https://gist.github.com/PurpleBooth/b24679402957c63ec426) for details on our code of conduct, and the process for submitting pull requests to us. ## Versioning We use [SemVer](http://semver.org/) for versioning. For the versions available, see the [tags on this repository](https://github.com/your/project/tags). ## Authors * **Billie Thompson** - *Initial work* - [PurpleBooth](https://github.com/PurpleBooth) See also the list of [contributors](https://github.com/your/project/contributors) who participated in this project. ## License This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details ## Acknowledgments * Hat tip to anyone whose code was used * Inspiration * etc Reference: [https://gist.github.com/PurpleBooth/109311bb0361f32d87a2",
    "url": "http://localhost:4000/playbook/docs/Standards/README/",
    "relUrl": "/docs/Standards/README/"
  },
  "21": {
    "id": "21",
    "title": "Definitions of Ready",
    "content": "Definitions of Ready REACH ALL BY DIVISION BY STACK LEADS Head of Software Engineering     A task is not ready for development unless it has satisfied the following: Acceptance Criteria clearly defined and describes all features of the story Product Owner has signed off on the User Story and Acceptance Criteria Acceptance test cases have been provided UX Designs, if applicable, have been completed and attached to the ticket No external or technical dependencies prevent the story from being completed The backlog item is estimated/sized by the delivery team and can be completed in a single sprint",
    "url": "http://localhost:4000/playbook/docs/Standards/Ready/",
    "relUrl": "/docs/Standards/Ready/"
  },
  "22": {
    "id": "22",
    "title": "Secure Code Standards",
    "content": "Secure Code Standards Developments teams should be trained on secure coding standards. Best practices guidelines per language must be read and understood. JavaScript: https://checkmarx.gitbooks.io/js-scp/ Java: https://www.oracle.com/technetwork/java/seccodeguide-139067.html iOS: https://developer.apple.com/library/archive/documentation/Security/Conceptual/SecureCodingGuide/Introduction.html Android: https://wiki.sei.cmu.edu/confluence/display/android/Android+Secure+Coding+Standard",
    "url": "http://localhost:4000/playbook/docs/Security/SecureCode/",
    "relUrl": "/docs/Security/SecureCode/"
  },
  "23": {
    "id": "23",
    "title": "Security",
    "content": "Security REACH ALL BY DIVISION BY STACK LEADS BISO     Key Rules: No PI Data in Source Code No credentials in Source Code Pen Test performed, shared and issues addressed for every major release. Password complexity and rotation policy in place and mandated. Secure Code Training completed and certification required to approve pull requests. All builds performed via CI No production access for any developer. Use the following guidelines to help reduce you attack surface: Validate input. Validate input from all untrusted data sources. Proper input validation can eliminate the vast majority of software vulnerabilities. Be suspicious of most external data sources, including command line arguments, network interfaces, environmental variables, and user-controlled files Heed compiler warnings. Compile code using the highest warning level available for your compiler and eliminate warnings by modifying the code. Use static and dynamic analysis tools to detect and eliminate additional security flaws. Architect and design for security policies. Create a software architecture and design your software to implement and enforce security policies. For example, if your system requires different privileges at different times, consider dividing the system into distinct intercommunicating subsystems, each with an appropriate privilege set. Keep it simple. Keep the design as simple and small as possible. Complex designs increase the likelihood that errors will be made in their implementation, configuration, and use. Additionally, the effort required to achieve an appropriate level of assurance increases dramatically as security mechanisms become more complex. Default deny. Base access decisions on permission rather than exclusion. This means that, by default, access is denied and the protection scheme identifies conditions under which access is permitted. Adhere to the principle of least privilege. Every process should execute with the least set of privileges necessary to complete the job. Any elevated permission should only be accessed for the least amount of time required to complete the privileged task. This approach reduces the opportunities an attacker has to execute arbitrary code with elevated privileges. Sanitize data sent to other systems. Sanitize all data passed to complex subsystems such as command shells, relational databases, and commercial off-the-shelf (COTS) components. Attackers may be able to invoke unused functionality in these components through the use of SQL, command, or other injection attacks. This is not necessarily an input validation problem because the complex subsystem being invoked does not understand the context in which the call is made. Because the calling process understands the context, it is responsible for sanitizing the data before invoking the subsystem. Practice defense in depth. Manage risk with multiple defensive strategies, so that if one layer of defense turns out to be inadequate, another layer of defense can prevent a security flaw from becoming an exploitable vulnerability and/or limit the consequences of a successful exploit. Use effective quality assurance techniques. Good quality assurance techniques can be effective in identifying and eliminating vulnerabilities. Fuzz testing, penetration testing, and source code audits should all be incorporated as part of an effective quality assurance program. Independent security reviews can lead to more secure systems. External reviewers bring an independent perspective; for example, in identifying and correcting invalid assumptions. Adopt a secure coding standard. Develop and/or apply a secure coding standard for your target development language and platform. Reference: https://wiki.sei.cmu.edu/confluence/display/seccode/Top+10+Secure+Coding+Practices",
    "url": "http://localhost:4000/playbook/docs/security",
    "relUrl": "/docs/security"
  },
  "24": {
    "id": "24",
    "title": "Security training",
    "content": "Security training It is expected that all Developers attend secure code training using [Secure Code Warrior]. The minimum requirement is to achieve the level of “Security Aware”.",
    "url": "http://localhost:4000/playbook/docs/Security/SecurityTraining/",
    "relUrl": "/docs/Security/SecurityTraining/"
  },
  "25": {
    "id": "25",
    "title": "Software Composition Analysis",
    "content": "Software Composition Analysis Software Composition Analysis (SCA) allows you to identify third-party and open source components that have been integrated into all your applications. It informs you about the licenses for each of them an identifies out-of-date libraries that should be upgraded or patched. As nearly all projects use some form of open source or third-party library it is required for software to use an SCA solution. It is expected that SCA forms part of your continuous integration build. Where available, use SCA plugins to your IDE in order to identify library issues early in the development cycle. The default solution is to use either Veracode SourceClear or NexusIQ, any other solutions must be approved by the Cyber team.",
    "url": "http://localhost:4000/playbook/docs/Security/SoftwareComposition/",
    "relUrl": "/docs/Security/SoftwareComposition/"
  },
  "26": {
    "id": "26",
    "title": "Software Engineering Checklist",
    "content": "Software Engineering Checklist As part of the Software Engineering Maturity Matrix we have provided a checklist around minimum standards for Software Engineering practices. Data Management Dev and Production schema closely matches QA can easily generate test data Database changes are scripted Automated schema changes via deployment Test data is anonymous Test data accurately represents production Production database is monitored Automated DB performance monitored and alerts Continuous Integration Continuous integration is used for all builds Any build can be recreated from source control Build metrics are visible and acted upon Red builds are never deployed Quality Dedicated QA function Documented regression tests User Stories have acceptance criteria 90% Unit Test Coverage for all new code Automated regression tests on key flows Environments Minimum of three environments: Dev, UAT and Production UAT environment matches production settings Creation of environments are trivial Non-production switched off when not in use Production environments are immutable Version Control Code is version controlled Basic branching standards Documented standards of version control Advanced git commands used and understood Branch level permission are set Code commits match user stories Pull requests have comments Short lived branches All Pull requests have 2 approvers Coding Standards Documented coding standards Coding design patterns have been established Pull requests are met with rigour Unit tests in code Linting is enabled in CI Code scanning linting IDE plugins adopted and used Coding design patterns are universally adopted Release Management Deployment is scripted Release notes are created Deployments are able to immediately be rolled back Some feature flags are used Security Application and system logging Documented secure coding standards Key Management System (credentials) Source code access using MFA Penetration testing Automated code analysis scan by CI Software Composition Analysis by CI",
    "url": "http://localhost:4000/playbook/docs/SoftwareEngChecklist/SoftwareEngChecklist/",
    "relUrl": "/docs/SoftwareEngChecklist/SoftwareEngChecklist/"
  },
  "27": {
    "id": "27",
    "title": "Stand-ups",
    "content": "Stand-ups Stand-ups or scrums are an invaluable form of communication, especially in distributed environments. Key to successful stand-ups include: Actually standing Meetings should start promptly. If key stakeholders are missing, start Ensure key themes are covered including what you need yesterday, plans for today and any blockers identified Any discussion lasting more than a minute should be parked Ensure attendees are actively listening and not waiting their turn to speak Discuss stand-up performance in your Retros",
    "url": "http://localhost:4000/playbook/docs/OperationalExcellence/StandUps/",
    "relUrl": "/docs/OperationalExcellence/StandUps/"
  },
  "28": {
    "id": "28",
    "title": "Standards",
    "content": "Standards Technical standards, an established norm or requirement about technical systems.",
    "url": "http://localhost:4000/playbook/docs/standards",
    "relUrl": "/docs/standards"
  },
  "29": {
    "id": "29",
    "title": "Static Code Analysis",
    "content": "Static Code Analysis All applications must use a Static Code Analysis(SCA) before going into production. SCA should be initiative as part of the CI build cycle but not required to synchronous that would make the build go red. Veracode is the preferred solution, any other solutions must be approved by the Cyber team.",
    "url": "http://localhost:4000/playbook/docs/Security/StaticCode/",
    "relUrl": "/docs/Security/StaticCode/"
  },
  "30": {
    "id": "30",
    "title": "Technical Design Review",
    "content": "Technical Design Review This happens before a technical story is started. The aim is to discuss your solution approach. The aim is: Review 1 page solution outline Review with one Developer and QA Discuss Design Patterns Define Data Contracts Define and verify Acceptance Criteria.",
    "url": "http://localhost:4000/playbook/docs/Standards/TechnicalDesignReview/",
    "relUrl": "/docs/Standards/TechnicalDesignReview/"
  },
  "31": {
    "id": "31",
    "title": "Test Data",
    "content": "Test Data Test data in Dev and UAT environments should not have any personally identifiable information (PII) data. Production data may be used in UAT environments, for any commercial",
    "url": "http://localhost:4000/playbook/docs/Testing/TestData/",
    "relUrl": "/docs/Testing/TestData/"
  },
  "32": {
    "id": "32",
    "title": "Test-Driven Development Checklist",
    "content": "Test-Driven Development Checklist TDD is described by a basic red-green-refactor cycle, constantly repeated to add new features or fix bugs. Red The development of every new feature should start with a failing test. Have you checked in the code in your remote or local repository? In case the code breaks, a revert is faster than a rewrite. Have you already written some production code? If so, comment it or (best) delete it to not be implicitly tied to an Api while writing the test. Have you chosen the right unit to expand? The modified class should be the one that remains more cohesive after the change, and often in new classes should be introduced instead of accommodating functionalities in existing ones. Does the test fail? If not, rewrite the test to expose the lack of functionality. Does a subset of the test already fail? Is so, you can remove the surplus part of the test, avoiding verbosity; it can come back in different test methods. Does the test prescribe overly specific assertions or expectations? If so, lessen the mock expectations by not checking method calls order or how many times a method is called; improve the assertions by substituting equality matches with matches over properties of the result object. Does the test name describe its intent? Make sure it is not tied to implementation details and works as low-level documentation. How much can you change in an hypothetical implementation without breaking the test (making it brittle)? Is the failure message expressive about what is broken? Make sure it describes where the failing functionality resides, highlighting the right location if it breaks in the future. Are magic numbers and strings expressed as constants? Is there repeated code? Test code refactoring is easy when done early and while a test fails, since in this paradigm it is more important to keep it failing then to keep it passing. Green Enough production code should be written to make the test pass. Does the production code make the test pass? (Plainly obvious) Does a subset of the production code make the test pass? If so, you can comment or (best) remove the unnecessary production code. Any more lines you write are untested lines you&#39;ll have to read and maintain in the future. Every other specific action will be taken in the Refactor phase. Refactor Improve the structure of the code to ease future changes and maintenance. Does repeated code exist in the current class? Is the name of the class under test appropriate? Do the public and protected method names describe their intent? Are they readable? Rename refactorings are between the most powerful ones. Does repeated code exist in different classes? Is there a missing domain concept? You can extract abstract classes or refactor towards composition. At this high-level the refactoring should be also applied to the unit tests, and there are many orthogonal techniques you can apply so I won&#39;t describe them all here.. Reference: https://www.giorgiosironi.com/2010/03/tdd-checklist-red-green-refactor-in.html",
    "url": "http://localhost:4000/playbook/docs/Testing/TestDrivenChecklist/",
    "relUrl": "/docs/Testing/TestDrivenChecklist/"
  },
  "33": {
    "id": "33",
    "title": "Test-Driven Development",
    "content": "Test-Driven Development All code must have unit test coverage that covers all functionality of the application. For legacy systems predating TDD we expect best-efforts, all new code must have unit tests. The benefits of Test-Driven Development include: Constant feedback that each component is still working. The unit tests act as documentation that cannot go out-of-date, unlike separate documentation, which can and frequently does. When the test passes and the production code is refactored to remove duplication, it is clear that the code is finished, and the developer can move on to a new test. Test-driven development forces critical analysis and design because the developer cannot create the production code without truly understanding what the desired result should be and how to test it. The software tends to be better designed, that is, loosely coupled and easily maintainable, because the developer is free to make design decisions and refactor at any time with confidence that the software is still working. This confidence is gained by running the tests. The need for a design pattern may emerge, and the code can be changed at that time. The test suite acts as a regression safety net on bugs: If a bug is found, the developer should create a test to reveal the bug and then modify the production code so that the bug goes away and all other tests still pass. On each successive test run, all previous bug fixes are verified. Reduced debugging time!",
    "url": "http://localhost:4000/playbook/docs/Testing/TestDrivenDevelopment/",
    "relUrl": "/docs/Testing/TestDrivenDevelopment/"
  },
  "34": {
    "id": "34",
    "title": "Testing",
    "content": "Testing REACH ALL BY DIVISION BY STACK LEADS Head of QA QA   Good Developers test this own work. Quality is the responsibility of the Developer and self-testing should be across the end-to-end of the user flow. The Developer and QA form a quality team that encompasses unit, regression, acceptance, performance, security, stress, integration, end-to-end testing are performed. Full testing guidelines are covered under the Quality Assurance Playbook",
    "url": "http://localhost:4000/playbook/docs/testing",
    "relUrl": "/docs/testing"
  },
  "35": {
    "id": "35",
    "title": "Version Control",
    "content": "Version Control REACH ALL BY DIVISION BY STACK LEADS Head of Software Engineering     We use a distributed version control with open source Git. Repository management of our internal code is via Atlassian Bitbucket or GitHub. Repositories are private by default; any public repositories need to have written permission by the Chief Officer Commercial at Qantas or the CTO of Qantas. Repos should be hosted in the Qantas account. It is not permissible for teams to have their own source control unless agreed to in writing. By default version control setting MFA enabled for all users Forking is turned off Release branch access is restricted Minimum of two pull requests into release branch Branching Branching Strategy We recommend four branching strategies depending on team and application maturity. Feature Branching: Default option for new teams and rapidly emerging environments GitHub Flow: Rapid development with frequent deployment Git Flow: For large teams and monolith applications Trunk Based: For advanced teams with high unit test coverage and automation. Ideally only use if intended for a continuous delivery environment Branch Naming The naming of a branch should be in the format: [Issue type]/[JIRA issue ID]-short-hyphen-case-description Issue Types release/ for release feature/ for features bugfix/ for bug fixes refactor/ for improvements/refactor improvement/ for random tasks hotfix/ for bug fixes which need to be branched from and merged into release branch Commit naming Commit messages should point out objectively what changes were made to the system using preferably bullet points with the goal, reason or intent rather than what has been done (the diff will show that). Messages should be clear and concise so that other programmers can look it up quickly. Good practices about commit messages can be found at tpope&#39;s blog and Chris Beam’s blog. All commits on a feature branch should be squashed into a single commit that contains the JIRA ticket number and a meaningful release-notes-like comment in the commit message. Commit Example Commit messages should be in the following format: [JIRA issue ID] [Summary of the changes.] Summary of the changes should be like the following: Did &lt;What change did you do?&gt;, so that &lt;Why did you do that (The reason)&gt; Remove &lt;troublesome gem X&gt;, which was causing &lt;specific description of issue introduced by gem&gt; &gt; BAD. Don&#39;t do this. Fixed compiling errors &gt; Good. QAN-1234 Added kangaroo to Qantas logo to keep it consistent with Qantas brand. Technical Design Review This happens before a technical story is started. The aim is to discuss your solution approach. The aim is: Review 1 page solution outline Review with one Developer and QA Discuss Design Patterns Define Data Contracts Define and verify Acceptance Criteria. Code Reviews/Pull Requests All production code needs to be reviewed via a pull request. Code reviews are classless: seniority is not required for a peer review although repository ownership can mandate approver authority. The aim of pull requests are to: Catch major defects before they are introduced into the existing code base Discuss architectural improvements and provide guidance for changes Look for secure coding practices Establish and follow coding standards Give kudos for great work Review Unit Tests Learn from other developers Cross Skill yourself Pull Request Social Contract Before each review, the Social Contract should be considered: We respect each other at all times We assume good intent Comments are contextual; focused only on the code Every comment should be addressed, never ignored Every peer review is an opportunity of collective learning and collaboration Pull Request Preparation Pull Requests should meet the following criteria: Should have description of changes Pull requests should be as small as possible with limited numbers of scope changes Build must be green and passed tests Reviewers should mark the pull request as “need work” even if they think the proposed changes are minor Should have description of changes in pull request Include all who are involved with that project Should squash unmeaningful commit messages before merge into master Include a brief on what the change is about. Should include unit test Pull Request Reviewers The guidelines for Pull Request Reviewers are: Mandatory 2 approvers (should not include members if pair programmed) Ideally 1 reviewer outside your team. Reviewed within two working days Requested refactoring changes should not change behaviour",
    "url": "http://localhost:4000/playbook/docs/Standards/VersionControl/",
    "relUrl": "/docs/Standards/VersionControl/"
  },
  "36": {
    "id": "36",
    "title": "Versioning",
    "content": "Versioning REACH ALL BY DIVISION BY STACK LEADS Head of Software Engineering     Versioning is standardised across all applications in the group. We follow the guidelines as set out in Semantic Versioning 2.0.0 (https://semver.org) Given a version number MAJOR.MINOR.PATCH, increment the: MAJOR version when you make incompatible API changes, MINOR version when you add functionality in a backwards-compatible manner, and PATCH version when you make backwards-compatible bug fixes. Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format.",
    "url": "http://localhost:4000/playbook/docs/Standards/Versioning/",
    "relUrl": "/docs/Standards/Versioning/"
  },
  "37": {
    "id": "37",
    "title": "Conventions Used",
    "content": "Conventions Used There are a number of conventions used through the Playbook. There are many links to internal reference documents such as the Confluence wiki. For access to the wiki send a request to support@servicerocket.com.au REACH ALL BY DIVISION BY STACK LEADS Head of Software Engineering     There are many links to internal reference documents such as the Confluence wiki. Code blocks are written in the format. ReactDOM.render( &lt;h1&gt;Hello, world!&lt;/h1&gt;, document.getElementById(&#39;root&#39;) );",
    "url": "http://localhost:4000/playbook/docs/conventions",
    "relUrl": "/docs/conventions"
  },
  "38": {
    "id": "38",
    "title": "Home",
    "content": "Purpose The purpose of the Software Engineering Playbook is to share the way of the working amongst the various development teams across the Qantas Group. Whilst our default view is to standardise amongst teams, we embrace diversity in thinking and approach. The Playbook should be thought of as a framework and not a standard. Teams should default to the Playbook but select to opt out where you believe the team can justify the need to be different.",
    "url": "http://localhost:4000/playbook/",
    "relUrl": "/"
  }
  
}
